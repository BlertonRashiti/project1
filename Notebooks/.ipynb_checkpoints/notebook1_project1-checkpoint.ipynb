{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{w}^{(t+1)}=\\boldsymbol{w}^{(t)}- \\gamma \\nabla \\mathcal{L}(\\boldsymbol{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood for logistic regression: $\\mathcal{L}(\\boldsymbol{w})=\\prod_{n=1}^N \\pi_{n}^{y_n} (1-\\pi_{n})^{1-y_n}$, where $\\pi_n(\\boldsymbol{x_n})=\\frac{e^{\\boldsymbol{x_n}^T\\boldsymbol{w}}}{1+e^{\\boldsymbol{x_n}^T\\boldsymbol{w}}} $\n",
    "Log-likelihood: $l(\\boldsymbol{w})=\\sum_{n=1}^N(y_n\\boldsymbol{x_n}^T\\boldsymbol{w}-log(1+e^{\\boldsymbol{x_n}^T\\boldsymbol{w}}))$\n",
    "\n",
    "Log-likelihood is differenetiable and concave, all local maxima are global maxima. Use gradient descent or SGD to optimize.\n",
    "\n",
    "Gradient: $\\nabla l(\\boldsymbol{w})=[\\frac{\\partial l}{\\partial w_1},\\dots, \\frac{\\partial l}{\\partial w_D}]^T =\n",
    "[\\sum_{n=1}^N(y_n-\\frac{e^{\\boldsymbol{x_n}^T\\boldsymbol{w}}}{1+e^{\\boldsymbol{x_n}^T\\boldsymbol{w}}})x_{n1},\\dots,\\sum_{n=1}^N(y_n-\\frac{e^{\\boldsymbol{x_n}^T\\boldsymbol{w}}}{1+e^{\\boldsymbol{x_n}^T\\boldsymbol{w}}})x_{nD}]^T=\n",
    "\\sum_{n=1}^N(y_n-\\frac{e^{\\boldsymbol{x_n}^T\\boldsymbol{w}}}{1+e^{\\boldsymbol{x_n}^T\\boldsymbol{w}}})\\boldsymbol{x_{n}}^T$\n",
    "\n",
    "Maximize the previous quantity is equivalent to use the following gradient via GD SGD\n",
    "$\\sum_{n=1}^N(\\frac{e^{\\boldsymbol{x_n}^T\\boldsymbol{w}}}{1+e^{\\boldsymbol{x_n}^T\\boldsymbol{w}}}-y_n)\\boldsymbol{x_{n}}^T=\\sum_{n=1}^N(\\sigma({\\boldsymbol{x_n}^T\\boldsymbol{w}})-y_n)\\boldsymbol{x_{n}}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x= pow(x,2.718281828459045)/(1+pow(x,2.718281828459045)) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_deriv_w(y, X, w, i):\n",
    "    S=0\n",
    "    for n in range(X.shape[0]):\n",
    "        S+=(sigmoid(np.dot(X[n,:],w))-y[n])*X[n,i]\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, X, w):\n",
    "    D=w.shape[0]\n",
    "    L=np.zeros(D)\n",
    "    for n in range(D):\n",
    "        L+=(sigmoid(np.dot(X[n,:],w))-y[n])*X[n,:]\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(y, X, w):\n",
    "    D=w.shape[0]\n",
    "    L=np.zeros(D)\n",
    "    for i in range(D):\n",
    "        L[i]=partial_deriv_w(y,X,w,i)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-492a2314bc0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#X=np.array([[9,0,11],[1,0,1],[1,0,1]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#w0=np.array([-1,1,1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def gradient_descent(y, X, w0, gamma, max_iters):\n",
    "    w=w0\n",
    "    for n in range(max_iters):\n",
    "        grad = gradient(y,X,w)\n",
    "        #print(grad)\n",
    "        w = w - gamma*grad\n",
    "        print(w)\n",
    "    return w\n",
    "\n",
    "#y=np.array([1,0,1])\n",
    "#X=np.array([[9,0,11],[1,0,1],[1,0,1]])\n",
    "#w0=np.array([-1,1,1])\n",
    "y=np.array([1,0,1,7])\n",
    "X=np.array([[9,0,11],[1,0,1],[1,0,1],[1,2,3]])\n",
    "w0=np.array([0,0,1000])\n",
    "max_iters=10000\n",
    "gamma=0.1\n",
    "#gradient_descent(y, X, w0, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.,   0., -12.])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent1(y, X, w0, gamma, max_iters):\n",
    "    w=w0\n",
    "    for n in range(max_iters):\n",
    "        grad = compute_gradient(y,X,w)\n",
    "        #print(grad)\n",
    "        w = w - gamma*grad\n",
    "        #print(w)\n",
    "    return w\n",
    "\n",
    "y=np.array([1,0,1,7])\n",
    "X=np.array([[9,0,11],[1,0,1],[1,0,1],[1,2,3]])\n",
    "w0=np.array([0,90,0])\n",
    "#y=np.array([1,0,1])\n",
    "#X=np.array([[9,0,11],[1,0,1],[1,0,1]])\n",
    "#w0=np.array([-1,1,1])\n",
    "max_iters=1000\n",
    "gamma=0.1\n",
    "gradient_descent1(y, X, w0, gamma, max_iters)\n",
    "compute_gradient(y,X,w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    w=gradient_descent(y, tx, initial_w, gamma, max_iters)\n",
    "    S=0\n",
    "    for n in range(tx.shape[0]):\n",
    "        z=np.dot(tx[n,:],w)\n",
    "        z=pow(z,2.718281828459045)\n",
    "        z=np.log(1+z)\n",
    "        m=y[n]*np.dot(tx[n,:],w)\n",
    "        print(z)\n",
    "        print(m)\n",
    "        S+= z-m\n",
    "    return (w,S)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.02434801244219\n",
      "13.247193017189197\n",
      "0.7020951594542215\n",
      "0.0\n",
      "0.7020951594542215\n",
      "1.0065758880582614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-1.08742912,  0.        ,  2.09400501]), -5.8252305738968255)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array([1,0,1])\n",
    "tx=np.array([[9,0,11],[1,0,1],[1,0,1]])\n",
    "initial_w=np.array([-1,0,2])\n",
    "max_iters=1000\n",
    "gamma=0.1\n",
    "\n",
    "logistic_regression(y, tx, initial_w, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
